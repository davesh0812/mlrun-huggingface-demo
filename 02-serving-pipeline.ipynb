{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Serving Pipeline\n",
    "\n",
    "This notebook demonstrate how to serve standard ML/DL models using **MLRun Serving**.\n",
    "\n",
    "Make sure you went over the basics in MLRun [**Quick Start Tutorial**](./01-mlrun-basics.html).\n",
    "\n",
    "\n",
    "MLRun serving can produce managed real-time serverless pipelines from various tasks, including MLRun models or standard model files.\n",
    "The pipelines use the Nuclio real-time serverless engine, which can be deployed anywhere.\n",
    "[Nuclio](https://nuclio.io/) is a high-performance open-source \"serverless\" framework that's focused on data, I/O, and compute-intensive workloads.\n",
    "\n",
    "\n",
    "MLRun serving supports advanced real-time data processing and model serving pipelines.<br>\n",
    "For more details and examples, see the {ref}`MLRun serving pipelines <serving>` documentation.\n",
    "\n",
    "Tutorial steps:\n",
    "- [**Create, test and build advanced model Serving Graph**](#serving-function)\n",
    "- [**Deploy the serving Function**](#deploy-serving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-01-30 11:48:34,028 [info] loaded project huggingface-demo from MLRun DB\n"
     ]
    }
   ],
   "source": [
    "import mlrun\n",
    "# get/create a project and register the data prep and trainer function in it\n",
    "project = mlrun.get_or_create_project(\n",
    "    name=\"huggingface-demo\", context=\"./\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"serving-function\"></a>\n",
    "## Create and test the Serving Function\n",
    "\n",
    "The serving function kind is using [Nuclio](https://nuclio.io/): a high-performance serverless event \n",
    "and data processing open-source platform. With just a few lines of code we can take our model, **expose** it with an **http endpoint** and deploy it on \n",
    "**high-performance** infrastructure that can easily scale up to serve on a **production environment** with hundreds of \n",
    "thousands of requests per second. To read more about serving functions in MLRun and some cool advanced features like model routers, error handling and more, please refer to our [documentation](https://docs.mlrun.org/en/latest/serving/serving-graph.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_function = mlrun.new_function(\"serving\", kind=\"serving\", image=\"yonishelach/ml-models:huggingface-demo-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the topology and get the graph object:\n",
    "graph = serving_function.set_topology(\"flow\", engine=\"async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: mlrun&#45;flow Pages: 1 -->\n",
       "<svg width=\"311pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 310.88 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>mlrun&#45;flow</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-40 306.8816,-40 306.8816,4 -4,4\"/>\n",
       "<!-- _start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_start</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"38.5476,-.0493 40.698,-.1479 42.8263,-.2953 44.9236,-.4913 46.9815,-.7353 48.9917,-1.0266 50.9463,-1.3645 52.8377,-1.7479 54.6587,-2.1759 56.4025,-2.6472 58.0628,-3.1606 59.634,-3.7147 61.1107,-4.308 62.4882,-4.9388 63.7625,-5.6054 64.9302,-6.3059 65.9882,-7.0385 66.9343,-7.8012 67.7669,-8.5918 68.4849,-9.4082 69.0878,-10.2481 69.5758,-11.1093 69.9496,-11.9894 70.2102,-12.886 70.3595,-13.7965 70.3997,-14.7186 70.3334,-15.6497 70.1636,-16.5873 69.8937,-17.5287 69.5276,-18.4713 69.0691,-19.4127 68.5225,-20.3503 67.8923,-21.2814 67.1831,-22.2035 66.3996,-23.114 65.5464,-24.0106 64.6285,-24.8907 63.6504,-25.7519 62.617,-26.5918 61.5329,-27.4082 60.4024,-28.1988 59.2299,-28.9615 58.0197,-29.6941 56.7755,-30.3946 55.5012,-31.0612 54.2002,-31.692 52.8757,-32.2853 51.5309,-32.8394 50.1684,-33.3528 48.7908,-33.8241 47.4003,-34.2521 45.9989,-34.6355 44.5886,-34.9734 43.1708,-35.2647 41.7472,-35.5087 40.3189,-35.7047 38.8872,-35.8521 37.4531,-35.9507 36.0175,-36 34.5815,-36 33.146,-35.9507 31.7119,-35.8521 30.2801,-35.7047 28.8519,-35.5087 27.4282,-35.2647 26.0105,-34.9734 24.6001,-34.6355 23.1988,-34.2521 21.8083,-33.8241 20.4306,-33.3528 19.0681,-32.8394 17.7233,-32.2853 16.3989,-31.692 15.0979,-31.0612 13.8236,-30.3946 12.5794,-29.6941 11.3691,-28.9615 10.1967,-28.1988 9.0662,-27.4082 7.982,-26.5918 6.9486,-25.7519 5.9706,-24.8907 5.0526,-24.0106 4.1995,-23.114 3.4159,-22.2035 2.7067,-21.2814 2.0765,-20.3503 1.53,-19.4127 1.0715,-18.4713 .7053,-17.5287 .4355,-16.5873 .2657,-15.6497 .1993,-14.7186 .2395,-13.7965 .3888,-12.886 .6495,-11.9894 1.0232,-11.1093 1.5112,-10.2481 2.1141,-9.4082 2.8321,-8.5918 3.6647,-7.8012 4.6109,-7.0385 5.6689,-6.3059 6.8365,-5.6054 8.1108,-4.9388 9.4884,-4.308 10.9651,-3.7147 12.5362,-3.1606 14.1966,-2.6472 15.9404,-2.1759 17.7614,-1.7479 19.6528,-1.3645 21.6074,-1.0266 23.6176,-.7353 25.6755,-.4913 27.7728,-.2953 29.901,-.1479 32.0515,-.0493 34.2154,0 36.3837,0 38.5476,-.0493\"/>\n",
       "<text text-anchor=\"middle\" x=\"35.2995\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">start</text>\n",
       "</g>\n",
       "<!-- sentiment&#45;analysis -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>sentiment&#45;analysis</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"204.7403\" cy=\"-18\" rx=\"98.2828\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"204.7403\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">sentiment&#45;analysis</text>\n",
       "</g>\n",
       "<!-- _start&#45;&gt;sentiment&#45;analysis -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_start&#45;&gt;sentiment&#45;analysis</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M69.9973,-18C77.9608,-18 86.8429,-18 96.1431,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"96.3752,-21.5001 106.3752,-18 96.3752,-14.5001 96.3752,-21.5001\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f3155c46450>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun.feature_store.steps import DateExtractor\n",
    "model_server_class = 'src.onnx_model_server.ONNXModelServer'\n",
    "model_uri = project.get_artifact('optimized_model').uri\n",
    "# model_uri = project.get_artifact('trained_model').uri\n",
    "# trained_model\n",
    "\n",
    "# Build the serving graph:\n",
    "graph.to(class_name=model_server_class, name=\"sentiment-analysis\", model_path=model_uri).respond()\n",
    "\n",
    "# Plot to graph:\n",
    "graph.plot(rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.projects.project.MlrunProject at 0x7f3155c1c1d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.set_function(serving_function,  with_repo=True)\n",
    "project.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-01-30 11:48:34,437 [warning] run command, file or code were not specified\n",
      "> 2023-01-30 11:48:46,992 [info] downloading v3io:///projects/huggingface-demo/artifacts/trainer-train/0/tokenizer.zip to local temp file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-01-30 11:48:47,602 [info] model sentiment-analysis was loaded\n",
      "request {'id': '8983de7363fc45528f84929010486a04', 'model_name': 'sentiment-analysis', 'outputs': [array([[-0.4294689 ,  0.48161912]], dtype=float32)]}\n",
      "['The sentiment is POSITIVE', 'The prediction score is 0.48161911964416504']\n"
     ]
    }
   ],
   "source": [
    "# create a mock server (in memory simulator for the graph for testing)\n",
    "server = serving_function.to_mock_server()\n",
    "\n",
    "body = \"i love flying\"\n",
    "\n",
    "# simulate a user request and print the results\n",
    "response = server.test(path='/predict', body=body)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy-serving\"></a>\n",
    "## Deploy the serving Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-01-30 11:48:47,624 [info] Starting remote function deploy\n",
      "2023-01-30 11:48:47  (info) Deploying function\n",
      "2023-01-30 11:48:47  (info) Building\n",
      "2023-01-30 11:48:48  (info) Staging files and preparing base images\n",
      "2023-01-30 11:48:48  (info) Building processor image\n",
      "2023-01-30 11:52:18  (info) Build complete\n",
      "2023-01-30 11:52:38  (info) Function deploy complete\n",
      "> 2023-01-30 11:52:39,244 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-huggingface-demo-serving.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['huggingface-demo-serving-huggingface-demo.default-tenant.app.cto-office.iguazio-cd1.com/']}\n"
     ]
    }
   ],
   "source": [
    "# Deploy it:\n",
    "deployment = project.deploy_function(serving_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-01-30 11:52:39,280 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-huggingface-demo-serving.default-tenant.svc.cluster.local:8080/predict'}\n",
      "['The sentiment is POSITIVE', 'The prediction score is 0.4270628094673157']\n"
     ]
    }
   ],
   "source": [
    "response = deployment.function.invoke(path='/predict', body=body)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio front-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradio is a friendly web interface that we demonstrate here how to use easily for submitting predictions to our real-time pipeline and to get the results as well!\n",
    "\n",
    "For more information, please see [gradio page](https://gradio.app/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_url = deployment.outputs['endpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://d8ef07c1-c951-4758.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d8ef07c1-c951-4758.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentiment(text):\n",
    "    resp = requests.post(serving_url, json={\"text\": text})\n",
    "    return resp.json()\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    input_box = [gr.Textbox(label=\"Text to analyze\", placeholder=\"Please insert text\")]\n",
    "    output = [gr.Textbox(label=\"Sentiment analysis result\"), gr.Textbox(label=\"Sentiment analysis score\")]\n",
    "    greet_btn = gr.Button(\"Submit\")\n",
    "    greet_btn.click(fn=sentiment, inputs=input_box, outputs=output)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
